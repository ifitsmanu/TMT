
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-67835289-3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-67835289-3');
</script>

<!-- ======================================================================= -->
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<style type="text/css">
  body {
    /* font-family: "Titillium Web","HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; */
    font-family: "Avenir", sans-serif;
    font-weight:400;
    font-size:18px;
    margin-left: auto;
    margin-right: auto;
    width: 1100px;
  }

  h1 {
    font-weight:300;
  }

  .column {
    float: left;
    /* width: 50%; */
  }

  /* Clear floats after the columns */
  .row:after {
    content: "";
    display: table;
    clear: both;
  }
  
  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }

  #authors td {
    padding-bottom:5px;
    padding-top:30px;
  }
</style>


<html>
  <head>

	<title>Turbulence Mitigation Transformer</title>
	<meta property="og:title" content="Turbulence Mitigation Transformer" />
	<meta property="og:image" content="" />
	<meta property="og:image:width" content="3000" />
	<meta property="og:image:height" content="800" />
  </head>

  <body>
    <br><br><br><br>
          <center>
          	<span style="font-size:38px">Turbulence Mitigation Transformer</span>
            <br><br>
		  <table align=center width=800px>
	  			  <tr>
	  	        <td align=center width=100px>
	  					  <center>
	  						<span style="font-size:24px"><a href="http://xg416.github.io">Xingguang Zhang</a></span>
		  		  		</center>
		  		  	</td>
	  	        <td align=center width=100px>
                <center>
	  						<span style="font-size:24px"><a href="https://web.ics.purdue.edu/~mao114/">Zhiyuan Mao</a></span>
		  		  		</center>
		  		  	</td>
	  	        <td align=center width=100px>
	  					  <center>
	  						<span style="font-size:24px"><a href="https://web.ics.purdue.edu/~nchimitt/index.html">Nicholas Chimitt</a></span>
		  		  		</center>
		  		  	</td>
	  	        <td align=center width=100px>
	  					  <center>
	  						<span style="font-size:24px"><a href="https://engineering.purdue.edu/ChanGroup/stanleychan.html">Stanley Chan</a></span>
		  		  		</center>
		  		  	</td>
			  </table>

	  		  <table align=center width=650px style="font-size:20px;">
	  			  <tr>
              <td align=center width=150px>
                <center>
                  <span><a href="http://arxiv.org/abs/2207.06465"> [Paper]</a></span>
                  </center>
                  </td>
	  	          <td align=center width=150px>
	  					<center>
	  						<span style=""><a href="https://github.com/xg416/TMT"> [Code]</a></span>
		  		  		</center>
		  		  	  </td>
                <td align=center width=150px>
	  					<center>
	  						<span style=""><a href="https://engineering.purdue.edu/ChanGroup/project_turbulence.html"> [Relevant]</a></span>
		  		  		</center>
		  		  	  </td>
	  	          <td align=center width=200px>
                  <center>
                    <span style=""><a href="https://engineering.purdue.edu/ChanGroup/project_turbulence_TurbSim_v2.html"> [P2S Simulator]</a></span>
                    </center>
                  </td>
              </tr>
	  			  <tr>
			  </table>
        <!-- ======================================================================= <br>
        <span style="font-size:20px">under review</span> -->
      </center>

  		  <br><br>
  		  <table align=center width=1100px>
  			  <tr>
            <td width=1050px>
    					<center>
    	          <a href="figs/video_22.gif"><img src = "figs/video_22.gif" width="85%"></img></href></a><br>
  					  </center>
            </td>
          </tr>
  		  </table>
      	<br><br>

		  <hr>

      <!--  <table align=center>
        <div class='row'>
          <div class='column'>
            <center>
              <h1>Abstract</h1>
              <p style="width:560px; text-align:left; color:darkslategray; ;">This paper proposes a simple self-supervised approach for learning representations
                for visual correspondence from raw video. We cast correspondence as link prediction in a space-time graph constructed from a video. In this graph, the nodes
                are patches sampled from each frame, and nodes adjacent in time can share a
                directed edge. We learn a node embedding in which pairwise similarity defines
                transition probabilities of a random walk. Prediction of long-range correspondence
                is efficiently computed as a walk along this graph. The embedding learns to guide
                the walk by placing high probability along paths of correspondence. Targets are
                formed without supervision, by cycle-consistency: we train the embedding to maximize the likelihood of returning to the initial node when walking along a graph
                constructed from a 'palindrome' of frames. We demonstrate that the approach allows
                for learning representations from large unlabeled video. Despite its simplicity,
                the method outperforms the self-supervised state-of-the-art on a variety of label
                propagation tasks involving objects, semantic parts, and pose. Moreover, we show
                that self-supervised adaptation at test-time and edge dropout improve transfer for
                object-level correspondence.</p>
            </center>
          </div>

          <div class="column" style='float:right; width:40%'>
            <center>
              <h1>Pseudo-code</h1>
              <img src = "figs/pseudocode.png" width="90%"></img>
            </center>
          </div>
        </div>
      </table>
      <br><br>
    </table> -->

      <center>
        <h1>Abstract</h1>
        <p> Restoring images distorted by atmospheric turbulence is a long-standing problem due to the spatially varying nature of the distortion, nonlinearity of the image formation process, and scarcity of training and testing data. Existing methods often have strong statistical assumptions on the distortion model which in many cases will lead to a limited performance in real-world scenarios as they do not generalize. To overcome the challenge, this paper presents an end-to-end physics-driven approach that is efficient and can generalize to real-world turbulence. On the data synthesis front, we significantly increase the image resolution that can be handled by the SOTA turbulence simulator by approximating the random field via wide-sense stationarity. The new data synthesis process enables the generation of large-scale multi-level turbulence and ground truth pairs for training. On the network design front, we propose the turbulence mitigation transformer (TMT), a two stage U-Net shaped multi-frame restoration network which has a noval efficient self-attention mechanism named temporal channel joint attention (TCJA). We also introduce a new training scheme that is enabled by the new simulator, and we design new transformer units to reduce the memory consumption. Experimental results on both static and dynamic scenes are promising, including various real turbulence scenarios.</p>
        <br><br>
		  </table>

		  <hr>

      <center>
        <h1>Results on real world static image sequences</h1>
        <table align=center width=1100px>
          <tr>
            <td align=center width=300px>
              <center>
                <a href="figs/pattern9.gif"><img src = "figs/pattern9.gif" width="85%"></img></href></a><br>
              </center>
            </td>
            <td align=center width=300px>
              <center>
              <a href="figs/pattern16.gif"><img src = "figs/pattern16.gif" width="85%"></img></href></a><br>
              </center>
            </td>
            <td align=center width=300px>
              <center>
              <a href="figs/pattern15.gif"><img src = "figs/pattern15.gif" width="85%"></img></href></a><br>
              </center>
            </td>
        </table>


        <table align=center width=1100px>
          <tr>
            <td align=center width=540px>
              <center>
                <a href="figs/pattern13.gif"><img src = "figs/pattern13.gif" width="90%"></img></href></a><br>
              </center>
            </td>
            <td align=center width=540px>
              <center>
              <a href="figs/pattern14.gif"><img src = "figs/pattern14.gif" width="90%"></img></href></a><br>
              </center>
            </td>
        </table>
        <p> Source of input images: 12 frames of the pattern 9, 13, 14, 15, 16 in the <a href="https://zenodo.org/communities/otis/?page=1&size=20"> OTIS</a> dataset </p>
      </center>
      <table align=center width=1080px>
        <tr>
          <td width=1080px>
            <center>
              <a href="figs/hillhouse.gif"><img src = "figs/hillhouse.gif" width="97%"></img></href></a><br>
            </center>
          </td>
        </tr>
        <br><br>
      </table>
      <p> Source of input images: first 12 frames of the hillhouse sequence in the <a href="https://arxiv.org/abs/2204.06989"> CLEAR's </a> dataset </p>
      <center>
        <table align=center width=1100px>
          <tr>
            <td align=center width=540px>
              <center>
                <a href="figs/pg2.gif"><img src = "figs/pg2.gif" width="90%"></img></href></a><br>
              </center>
            </td>
            <td align=center width=540px>
              <center>
              <a href="figs/pg24.gif"><img src = "figs/pg24.gif" width="90%"></img></href></a><br>
              </center>
            </td>
            <br><br>
        </table>
        <table align=center width=1100px>
          <tr>
            <td align=center width=540px>
              <center>
                <a href="figs/pg58.gif"><img src = "figs/pg58.gif" width="90%"></img></href></a><br>
              </center>
            </td>
            <td align=center width=540px>
              <center>
              <a href="figs/pg96.gif"><img src = "figs/pg96.gif" width="90%"></img></href></a><br>
              </center>
            </td>
            
        </table>
        <p> Source of input images: first 12 frames of the 2nd, 24th, 58th, 96th sequences in the <a href="http://cvpr2022.ug2challenge.org/dataset22_t3.html"> static text </a> dataset </p>
        <br><br>
      </center>

      <hr>


      <center>
      <h1>Results on real world dynamic image sequences</h1>
		  <table align=center width=1080px>
        <tr>
          <td align=center width=200px>
            <center>
            <span style="font-size:18px">Left: input sequence</span>
            </center>
          </td>
          <td align=center width=200px>
            <center>
            <span style="font-size:18px">Right: restored</span>
            </center>
          </td>
      </table>

      <table border="0" align="center" cellspacing="0" cellpadding="20">
        <video width="1080" height="320" controls muted>
          <source src="video/sh.mp4" type="video/mp4">
          <source src="video/sh.ogg" type="video/ogg">
        </video>
        <p> Source of the input video: the <a href="https://arxiv.org/abs/2204.06989"> CLEAR's </a> dataset </p>
        <br><br>
      </table>
      

      <table border="0" align="center" cellspacing="0" cellpadding="20">
        <video width="1080" height="320" controls muted>
          <source src="video/TMT_airport.mp4" type="video/mp4">
          <source src="video/TMT_airport.ogg" type="video/ogg">
        </video>
        <p> Source of the input video: the <a href="https://arxiv.org/abs/2204.06989"> CLEAR's </a> dataset </p>
      <br><br>
      </table>
      <table border="0" align="center" cellspacing="0" cellpadding="20">
        <video width="1080" height="263" controls muted>
          <source src="video/video_19.mp4" type="video/mp4">
          <source src="video/video_19.ogg" type="video/ogg">
        </video>
        <p> Source of the input video: the <a href="https://zenodo.org/record/5101910#.Ys2ak-yZOrc"> TSRWGAN's </a> dataset </p>
      <br><br>
      </table>
      <table border="0" align="center" cellspacing="0" cellpadding="20">
        <video width="1080" height="340" controls muted>
          <source src="video/video_23.mp4" type="video/mp4">
          <source src="video/video_23.ogg" type="video/ogg">
        </video>
        <p> Source of the input video: the <a href="https://zenodo.org/record/5101910#.Ys2ak-yZOrc"> TSRWGAN's </a> dataset </p>
      <br><br>
      </table>
      <table border="0" align="center" cellspacing="0" cellpadding="20">
        <video width="1024" height="480" controls muted>
          <source src="video/hillhouse.mp4" type="video/mp4">
          <source src="video/hillhouse.ogg" type="video/ogg">
        </video>
        <p> Source of the input video: the <a href="https://arxiv.org/abs/2204.06989"> CLEAR's </a> dataset </p>
      <br><br>
      </table>
      </center>

      <hr>
  		  <!-- <table align=center width=550px> -->
  		  <table align=center width=1100px>
	 		<center><h1>Network Architecture</h1></center>
       <center><span class="image"><img src="figs/model.png" alt="" width="967" height="319"/></span></center>
       <br><br>
  		  </table>
		  <br>

        <hr>

  		  <table align=center width=1100px>
  			  <tr>
  	        <td width=1000px>
  					<left>
          <center><h1>Data synthesis pipeline</h1></center>
          <center><span class="image"><img src="figs/simulation.png" alt="" width="967" height="313"/></span></center>
        </table>
    		  <br><br>
  		  </table>

			</left>
		</td>
			 </tr>
		</table>

		<br><br>


</body>
</html>
